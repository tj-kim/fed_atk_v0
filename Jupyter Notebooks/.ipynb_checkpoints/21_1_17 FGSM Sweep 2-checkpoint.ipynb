{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FGSM Sweep 2\n",
    "\n",
    "TJ Kim <br/>\n",
    "1/17/21\n",
    "\n",
    "#### Objective: \n",
    "Run FGSM attack on different on the neck including linear setting.\n",
    "\n",
    "First we simply test if the FSGM attack works as expected,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/satya_code\n"
     ]
    }
   ],
   "source": [
    "cd '/home/ubuntu/satya_code/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Relevant Libraries and Modules\n",
    "\n",
    "Load the relevant libraries for the federated learning code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import yaml\n",
    "        \n",
    "from femnist_dataloader import Dataloader\n",
    "from cnn_head import CNN_Head\n",
    "from cnn_neck import CNN_Neck\n",
    "from cnn_server import Server\n",
    "from cnn_client import Client\n",
    "from data_manager import DataManager\n",
    "from utils import cuda, where\n",
    "\n",
    "from utilities import freeze_layers\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import queue\n",
    "\n",
    "# Extra not from py file\n",
    "from collections import OrderedDict \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directly Make victim NN Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Victim_NN(nn.Module):\n",
    "    \"\"\"\n",
    "    Summary: \n",
    "    \n",
    "    Pytorch NN module that takes pre-trained weights from layered personalized model\n",
    "    We also load the data-loader and give test,attack functionality\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, head_network, neck_network, dataloader):\n",
    "        \n",
    "        # Init attributes\n",
    "        super(Victim_NN, self).__init__()\n",
    "        self.head = head_network\n",
    "        self.neck = neck_network\n",
    "        self.dataloader = dataloader\n",
    "        self.criterion = nn.NLLLoss()\n",
    "        \n",
    "        # test_acc attributes\n",
    "        self.orig_test_acc = None\n",
    "        self.adv_test_acc = None\n",
    "        \n",
    "        self.orig_output_sim = None\n",
    "        self.adv_output_sim = None\n",
    "        \n",
    "        # I_FGSM attributes\n",
    "        self.x_orig = None\n",
    "        self.x_adv = None\n",
    "        self.y_orig = None\n",
    "        self.target = None\n",
    "        \n",
    "        self.softmax_orig = None\n",
    "        self.output_orig = None\n",
    "        self.softmax_adv = None\n",
    "        self.output_adv = None\n",
    "        \n",
    "        self.orig_loss = None\n",
    "        self.adv_loss = None\n",
    "        self.orig_acc = None\n",
    "        self.adv_acc = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.neck.forward(x)\n",
    "        x = self.head.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward_transfer(self, x_orig, x_adv, y_orig, y_adv,\n",
    "                         true_labels, target, print_info = False):\n",
    "        \"\"\"\n",
    "        Assume that input images are in pytorch tensor format\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = y_orig.shape[0]\n",
    "        \n",
    "        # Forward Two Input Types\n",
    "        h_adv = self.forward(x_adv)\n",
    "        h_orig = self.forward(x_orig)\n",
    "        h_adv_category = torch.argmax(h_adv,dim = 1)\n",
    "        h_orig_category = torch.argmax(h_orig,dim = 1)\n",
    "        \n",
    "        # Record Different Parameters\n",
    "        self.orig_test_acc = (h_orig_category == true_labels).float().sum()/batch_size\n",
    "        self.adv_test_acc = (h_adv_category == true_labels).float().sum()/batch_size\n",
    "        \n",
    "        self.orig_output_sim = (h_orig_category == y_orig).float().sum()/batch_size\n",
    "        self.adv_output_sim = (h_adv_category == y_adv).float().sum()/batch_size\n",
    "        \n",
    "        self.orig_target_achieve = (h_orig_category == target).float().sum()/batch_size\n",
    "        self.adv_target_achieve = (h_adv_category == target).float().sum()/batch_size\n",
    "\n",
    "        \n",
    "        # Print Relevant Information\n",
    "        if print_info:\n",
    "            print(\"---- Attack Transfer:\", \"----\\n\")\n",
    "            print(\"         Orig Test Acc:\", self.orig_test_acc.item())\n",
    "            print(\"          Adv Test Acc:\", self.adv_test_acc.item())\n",
    "            print(\"Orig Output Similarity:\", self.orig_output_sim.item())\n",
    "            print(\" Adv Output Similarity:\", self.adv_output_sim.item())\n",
    "            print(\"       Orig Target Hit:\", self.orig_target_achieve.item())\n",
    "            print(\"        Adv Target Hit:\", self.adv_target_achieve.item())\n",
    "        \n",
    "    def i_fgsm(self, batch_size = 10, target= -1, eps=0.03, alpha=1, \n",
    "               iteration=1, x_val_min=-1, x_val_max=1, print_info=False):\n",
    "        \"\"\"\n",
    "        batch_size - number of images to adversarially perturb\n",
    "        targetted - target class output we desire to alter all inputs into\n",
    "        eps - max amount to add perturbations per pixel per iteration\n",
    "        alpha - gradient scaling (increase minimum perturbation amount below epsilon)\n",
    "        iteration - how many times to perturb\n",
    "        x_val_min/max - NN input valid range to keep perturbations within\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        # Load data to perturb\n",
    "    \n",
    "        image_data = self.dataloader.load_batch(batch_size)\n",
    "        self.x_orig  = torch.Tensor(image_data['input']).reshape(batch_size,1,28,28)\n",
    "        self.y_orig = torch.Tensor(image_data['label']).type(torch.LongTensor).cuda()\n",
    "        self.target = target\n",
    "        \n",
    "        self.x_adv = Variable(self.x_orig, requires_grad=True)\n",
    "        \n",
    "        for i in range(iteration):\n",
    "            \n",
    "            h_adv = self.forward(self.x_adv)\n",
    "            \n",
    "            # Loss function based on target\n",
    "            if target > -1:\n",
    "                target_tensor = torch.LongTensor(self.y_orig.size()).fill_(target)\n",
    "                target_tensor = Variable(cuda(target_tensor, self.cuda), requires_grad=False)\n",
    "                cost = self.criterion(h_adv, target_tensor)\n",
    "            else:\n",
    "                cost = -self.criterion(h_adv, self.y_orig)\n",
    "\n",
    "            self.zero_grad()\n",
    "\n",
    "            if self.x_adv.grad is not None:\n",
    "                self.x_adv.grad.data.fill_(0)\n",
    "            cost.backward()\n",
    "\n",
    "            self.x_adv.grad.sign_()\n",
    "            self.x_adv = self.x_adv - alpha*self.x_adv.grad\n",
    "            self.x_adv = where(self.x_adv > self.x_orig+eps, self.x_orig+eps, self.x_adv)\n",
    "            self.x_adv = where(self.x_adv < self.x_orig-eps, self.x_orig-eps, self.x_adv)\n",
    "            self.x_adv = torch.clamp(self.x_adv, x_val_min, x_val_max)\n",
    "            self.x_adv = Variable(self.x_adv.data, requires_grad=True)\n",
    "\n",
    "        self.softmax_orig = self.forward(self.x_orig)\n",
    "        self.output_orig = torch.argmax(self.softmax_orig,dim=1)\n",
    "        self.softmax_adv = self.forward(self.x_adv)\n",
    "        self.output_adv = torch.argmax(self.softmax_adv,dim=1)\n",
    "        \n",
    "        # Record accuracy and loss\n",
    "        self.orig_loss = self.criterion(self.softmax_orig, self.y_orig).item()\n",
    "        self.adv_loss = self.criterion(self.softmax_adv, self.y_orig).item()\n",
    "        self.orig_acc = (self.output_orig == self.y_orig).float().sum()/batch_size\n",
    "        self.adv_acc = (self.output_adv == self.y_orig).float().sum()/batch_size\n",
    "        \n",
    "        # Add Perturbation Distance (L2 norm) - across each input\n",
    "        self.norm = torch.norm(torch.sub(self.x_orig, self.x_adv, alpha=1),dim=(2,3))\n",
    "\n",
    "        # Print Relevant Information\n",
    "        if print_info:\n",
    "            print(\"---- FGSM Batch Size:\", batch_size, \"----\\n\")\n",
    "            print(\"Orig Target:\", self.y_orig.tolist())\n",
    "            print(\"Orig Output:\", self.output_orig.tolist())\n",
    "            print(\"ADV Output :\", self.output_adv.tolist(),'\\n')\n",
    "            print(\"Orig Loss  :\", self.orig_loss)\n",
    "            print(\"ADV Loss   :\", self.adv_loss,'\\n')\n",
    "            print(\"Orig Acc   :\", self.orig_acc.item())\n",
    "            print(\"ADV Acc    :\", self.adv_acc.item())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data and weights for no reason because it worked in the previous setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'individual_head_networks'#'exp3_neck2_1_head2'\n",
    "\n",
    "# Generate Head and Neck NN objects\n",
    "mode = 'cuda'\n",
    "head_nn = CNN_Head(mode)\n",
    "neck_nn = CNN_Neck(mode)\n",
    "\n",
    "# Which network to load and directory\n",
    "i = 0\n",
    "exp_path = \"Results/federated_system/\"+filename+\"/\"\n",
    "nn_path = exp_path + filename + \"_\"\n",
    "\n",
    "# Load pre-trained weights\n",
    "head_path = nn_path + str(i) +\"_head_network\"\n",
    "neck_path = nn_path + str(i) +\"_neck_network\"\n",
    "\n",
    "head = torch.load(head_path)\n",
    "neck = torch.load(neck_path)\n",
    "    \n",
    "head_edit = OrderedDict()\n",
    "neck_edit = OrderedDict()\n",
    "\n",
    "# Edit the ordered_dict key names to be torch compatible\n",
    "for key in head.keys():\n",
    "    head_edit[\"network.\"+key] = head[key]\n",
    "\n",
    "for key in neck.keys():\n",
    "    neck_edit[\"network.\"+key] = neck[key]\n",
    "\n",
    "head_nn.load_state_dict(head_edit)\n",
    "neck_nn.load_state_dict(neck_edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  all_data_12_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_20_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_11_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_18_niid_0_keep_0_train_9.json\n"
     ]
    }
   ],
   "source": [
    "# Obtain Information Regarding Dataset Slices\n",
    "with open(r'config.yaml') as file:\n",
    "        config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        \n",
    "file_indices = [i for i in range(config['num_sets'])]\n",
    "#random.shuffle(file_indices)\n",
    "client_slice = len(file_indices)//config['num_clients']\n",
    "\n",
    "# Load the relevant dataloader for this specific user (0)\n",
    "i = 0\n",
    "loader = Dataloader(file_indices,[i*(client_slice),min((i+1)*(client_slice),35)])  \n",
    "loader.load_training_dataset()\n",
    "loader.load_testing_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- FGSM Batch Size: 20 ----\n",
      "\n",
      "Orig Target: [34, 33, 6, 24, 34, 1, 36, 5, 7, 32, 50, 9, 27, 9, 7, 19, 53, 6, 2, 26]\n",
      "Orig Output: [34, 33, 6, 0, 34, 1, 36, 5, 7, 32, 0, 9, 27, 9, 7, 19, 53, 6, 2, 26]\n",
      "ADV Output : [34, 5, 6, 12, 5, 5, 5, 15, 5, 32, 5, 5, 5, 28, 18, 5, 5, 5, 5, 5] \n",
      "\n",
      "Orig Loss  : 0.3385593891143799\n",
      "ADV Loss   : 6.204596519470215 \n",
      "\n",
      "Orig Acc   : 0.9000000357627869\n",
      "ADV Acc    : 0.15000000596046448\n",
      "---- Attack Transfer: ----\n",
      "\n",
      "         Orig Test Acc: 0.9000000357627869\n",
      "          Adv Test Acc: 0.15000000596046448\n",
      "Orig Output Similarity: 1.0\n",
      " Adv Output Similarity: 1.0\n",
      "       Orig Target Hit: 0.05000000074505806\n",
      "        Adv Target Hit: 0.6500000357627869\n"
     ]
    }
   ],
   "source": [
    "victim_nn = Victim_NN(head_nn,neck_nn,loader)\n",
    "victim_nn.i_fgsm(batch_size = 20, target= 5, eps=0.1, alpha=0.1, \n",
    "               iteration=30, x_val_min=-1, x_val_max=1, print_info=True)\n",
    "\n",
    "victim_nn.forward_transfer(victim_nn.x_orig, victim_nn.x_adv, victim_nn.output_orig, victim_nn.output_adv,\n",
    "                         victim_nn.y_orig, victim_nn.target, print_info = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Attack Sweeping Different Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Config File and Slie Indices\n",
    "with open(r'config.yaml') as file:\n",
    "        config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        \n",
    "file_indices = [i for i in range(config['num_sets'])]\n",
    "#random.shuffle(file_indices)\n",
    "client_slice = len(file_indices)//config['num_clients']\n",
    "\n",
    "# File names of FL trained setting\n",
    "filenames = [\"exp3_neck2_0_head3\"]\n",
    "\n",
    "# Matrix to Record Performance\n",
    "orig_acc_transfers = np.zeros((1,config['num_clients']))\n",
    "orig_similarities = np.zeros((1,config['num_clients']))\n",
    "orig_target_hit = np.zeros((1,config['num_clients']))\n",
    "adv_acc_transfers = np.zeros((1,config['num_clients']))\n",
    "adv_similarities = np.zeros((1,config['num_clients']))\n",
    "adv_target_hit = np.zeros((1,config['num_clients']))\n",
    "\n",
    "# Attack Params\n",
    "batch_size = 1000\n",
    "eps = 1\n",
    "alpha = 1\n",
    "iteration = 1\n",
    "target = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_victim(idx, loader):\n",
    "    # Load the corresponding head/neck network in victim nn module \n",
    "    \n",
    "    mode = 'cuda'\n",
    "    #head_nn = CNN_Head(mode)\n",
    "    #neck_nn = CNN_Neck(mode)\n",
    "    \n",
    "    # Which network to load and directory\n",
    "    exp_path = \"Results/federated_system/\"+filename+\"/\"\n",
    "    nn_path = exp_path + filename + \"_\"\n",
    "\n",
    "    # Load pre-trained weights\n",
    "    head_path = nn_path + str(idx) +\"_head_network\"\n",
    "    neck_path = nn_path + str(idx) +\"_neck_network\"\n",
    "\n",
    "    head = torch.load(head_path)\n",
    "    neck = torch.load(neck_path)\n",
    "\n",
    "    head_edit = OrderedDict()\n",
    "    neck_edit = OrderedDict()\n",
    "\n",
    "    # Edit the ordered_dict key names to be torch compatible\n",
    "    for key in head.keys():\n",
    "        head_edit[\"network.\"+key] = head[key]\n",
    "\n",
    "    for key in neck.keys():\n",
    "        neck_edit[\"network.\"+key] = neck[key]\n",
    "\n",
    "    head_nn.load_state_dict(head_edit)\n",
    "    neck_nn.load_state_dict(neck_edit)\n",
    "    \n",
    "    return Victim_NN(head_nn,neck_nn,loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  all_data_12_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_20_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_11_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_18_niid_0_keep_0_train_9.json\n",
      "======== Source 0 ========\n",
      "    ==== Dest 0 ====\n",
      "---- Attack Transfer: ----\n",
      "\n",
      "         Orig Test Acc: 0.9040000438690186\n",
      "          Adv Test Acc: 0.0\n",
      "Orig Output Similarity: 1.0\n",
      " Adv Output Similarity: 1.0\n",
      "       Orig Target Hit: 0.0\n",
      "        Adv Target Hit: 0.0\n",
      "    ==== Dest 1 ====\n",
      "---- Attack Transfer: ----\n",
      "\n",
      "         Orig Test Acc: 0.8330000638961792\n",
      "          Adv Test Acc: 0.0010000000474974513\n",
      "Orig Output Similarity: 0.8740000128746033\n",
      " Adv Output Similarity: 0.1770000010728836\n",
      "       Orig Target Hit: 0.003000000026077032\n",
      "        Adv Target Hit: 0.0\n",
      "    ==== Dest 2 ====\n",
      "---- Attack Transfer: ----\n",
      "\n",
      "         Orig Test Acc: 0.8090000152587891\n",
      "          Adv Test Acc: 0.0010000000474974513\n",
      "Orig Output Similarity: 0.8480000495910645\n",
      " Adv Output Similarity: 0.30100002884864807\n",
      "       Orig Target Hit: 0.0010000000474974513\n",
      "        Adv Target Hit: 0.0\n",
      "    ==== Dest 3 ====\n",
      "---- Attack Transfer: ----\n",
      "\n",
      "         Orig Test Acc: 0.8750000596046448\n",
      "          Adv Test Acc: 0.0010000000474974513\n",
      "Orig Output Similarity: 0.89000004529953\n",
      " Adv Output Similarity: 0.32600000500679016\n",
      "       Orig Target Hit: 0.0\n",
      "        Adv Target Hit: 0.0\n",
      "    ==== Dest 4 ====\n",
      "---- Attack Transfer: ----\n",
      "\n",
      "         Orig Test Acc: 0.7470000386238098\n",
      "          Adv Test Acc: 0.004000000189989805\n",
      "Orig Output Similarity: 0.7930000424385071\n",
      " Adv Output Similarity: 0.2880000174045563\n",
      "       Orig Target Hit: 0.005000000353902578\n",
      "        Adv Target Hit: 0.0\n",
      "    ==== Dest 5 ====\n",
      "---- Attack Transfer: ----\n",
      "\n",
      "         Orig Test Acc: 0.8170000314712524\n",
      "          Adv Test Acc: 0.0010000000474974513\n",
      "Orig Output Similarity: 0.8400000333786011\n",
      " Adv Output Similarity: 0.25200000405311584\n",
      "       Orig Target Hit: 0.0\n",
      "        Adv Target Hit: 0.0\n",
      "    ==== Dest 6 ====\n",
      "---- Attack Transfer: ----\n",
      "\n",
      "         Orig Test Acc: 0.8230000138282776\n",
      "          Adv Test Acc: 0.004000000189989805\n",
      "Orig Output Similarity: 0.8600000143051147\n",
      " Adv Output Similarity: 0.20200000703334808\n",
      "       Orig Target Hit: 0.0020000000949949026\n",
      "        Adv Target Hit: 0.0\n",
      "    ==== Dest 7 ====\n",
      "---- Attack Transfer: ----\n",
      "\n",
      "         Orig Test Acc: 0.8750000596046448\n",
      "          Adv Test Acc: 0.0020000000949949026\n",
      "Orig Output Similarity: 0.8940000534057617\n",
      " Adv Output Similarity: 0.3100000023841858\n",
      "       Orig Target Hit: 0.0\n",
      "        Adv Target Hit: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for source in range(1):\n",
    "    \n",
    "    # Bring in the data loader for this client\n",
    "    loader = Dataloader(file_indices,[source*(client_slice),min((source+1)*(client_slice),35)])  \n",
    "    loader.load_training_dataset()\n",
    "    loader.load_testing_dataset()\n",
    "\n",
    "    victim_source = load_victim(source,loader,filename)\n",
    "\n",
    "    # Generate adversarial Perturbations\n",
    "    victim_source.i_fgsm(batch_size = batch_size, target= target, eps=eps, alpha=alpha, \n",
    "               iteration=iteration, x_val_min=-1, x_val_max=1, print_info=False)\n",
    "\n",
    "    # Record relevant tensors\n",
    "    x_orig = victim_source.x_orig\n",
    "    y_orig = victim_source.output_orig\n",
    "    y_true = victim_source.y_orig\n",
    "    x_adv = victim_source.x_adv\n",
    "    y_adv = victim_source.output_adv\n",
    "\n",
    "    print(\"======== Source\", source, \"========\")\n",
    "\n",
    "    for dest in range(config['num_clients']):\n",
    "\n",
    "        print(\"    ==== Dest\", dest, \"====\")\n",
    "\n",
    "        victim_dest = load_victim(dest,loader,filename)\n",
    "\n",
    "        # Compute Stats and record\n",
    "        victim_dest.forward_transfer(x_orig,x_adv,y_orig,y_adv,y_true, target, print_info=True)\n",
    "\n",
    "        orig_acc_transfers[source,dest] = victim_dest.orig_test_acc\n",
    "        orig_similarities[source,dest] = victim_dest.orig_output_sim\n",
    "        orig_target_hit[source,dest] = victim_dest.orig_target_achieve\n",
    "\n",
    "        adv_acc_transfers[source,dest] = victim_dest.adv_test_acc\n",
    "        adv_similarities[source,dest] = victim_dest.adv_output_sim\n",
    "        adv_target_hit[source,dest] = victim_dest.adv_target_achieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_acc_transfers\n",
      " [0.904 0.833 0.809 0.875 0.747 0.817 0.823 0.875]\n",
      "orig_similarities\n",
      " [[1.    0.874 0.848 0.89  0.793 0.84  0.86  0.894]]\n",
      "orig_target_hit\n",
      " [[0.    0.003 0.001 0.    0.005 0.    0.002 0.   ]]\n",
      "adv_acc_transfers\n",
      " [[0.    0.001 0.001 0.001 0.004 0.001 0.004 0.002]]\n",
      "adv_similarities\n",
      " [[1.    0.177 0.301 0.326 0.288 0.252 0.202 0.31 ]]\n",
      "adv_target_hit\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"orig_acc_transfers\\n\",np.round(orig_acc_transfers,3)[0])\n",
    "print(\"orig_similarities\\n\",np.round(orig_similarities,3))\n",
    "print(\"orig_target_hit\\n\",np.round(orig_target_hit,3))\n",
    "print(\"adv_acc_transfers\\n\",np.round(adv_acc_transfers,3))\n",
    "print(\"adv_similarities\\n\",np.round(adv_similarities,3))\n",
    "print(\"adv_target_hit\\n\",np.round(adv_target_hit,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
