{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Clients\n",
    "\n",
    "TJ Kim\n",
    "\n",
    "12.7.20\n",
    "\n",
    "### Summary:\n",
    "\n",
    "Load all 8 clients from the initial simulations. \n",
    "\n",
    "First we move into the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/satya_code\n"
     ]
    }
   ],
   "source": [
    "cd '/home/ubuntu/satya_code/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Relevant Libraries and Modules\n",
    "\n",
    "These are ripped from cnn_experiment.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import yaml\n",
    "        \n",
    "from femnist_dataloader import Dataloader\n",
    "from cnn_head import CNN_Head\n",
    "from cnn_neck import CNN_Neck\n",
    "from cnn_server import Server\n",
    "from cnn_client import Client\n",
    "from data_manager import DataManager\n",
    "\n",
    "from utilities import freeze_layers\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import queue\n",
    "\n",
    "# Extra not from py file\n",
    "from collections import OrderedDict \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a Client for Each Client Node\n",
    "\n",
    "Each client node will be initialized, and the weight will be back_filled with training sets and trained weights.\n",
    "\n",
    "First we load the relevant settings for generating clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "mode = 'cuda'\n",
    "\n",
    "with open(r'config.yaml') as file:\n",
    "        config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        \n",
    "file_indices = [i for i in range(config['num_sets'])]\n",
    "#random.shuffle(file_indices)\n",
    "client_slice = len(file_indices)//config['num_clients']\n",
    "    \n",
    "''' Objects to Hold Data Generated '''\n",
    "exp_path = \"Results/federated_system/individual_head_networks/\"\n",
    "dm = DataManager(exp_path)\n",
    "\n",
    "\n",
    "'''Filler information used to instantiate clients'''\n",
    "\n",
    "upload_schedule = []\n",
    "\n",
    "for i in range(config['iterations']*2):\n",
    "    clients = [str(i) for i in range(config['num_clients'])]\n",
    "    random.shuffle(clients)\n",
    "    client_ids = clients[:int(config['upload_fraction']*config['num_clients'])]\n",
    "    upload_schedule.append(client_ids)\n",
    "\n",
    "with open(exp_path+\"upload_schedule.txt\", 'w') as f:\n",
    "    for client_selection in upload_schedule:\n",
    "        f.write(str(client_selection) + '\\n')\n",
    "\n",
    "''' Instantiate Global Data Repo '''\n",
    "\n",
    "param_queue =  mp.Queue(maxsize=config['num_clients']*2)\n",
    "model_queue =  mp.Queue(maxsize=10)\n",
    "update_queue = mp.Queue(maxsize=1)\n",
    "    \n",
    "    \n",
    "neck_template = CNN_Neck(mode)\n",
    "head_template = CNN_Head(mode)\n",
    "\n",
    "neck_weights = neck_template.get_weights()\n",
    "head_weights = head_template.get_weights()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the clients are generated and datasets are divided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  all_data_12_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_20_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_11_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_18_niid_0_keep_0_train_9.json\n"
     ]
    }
   ],
   "source": [
    "''' Generate dummy clients with correct data set '''\n",
    "\n",
    "threads = []\n",
    "dataset_split = []\n",
    "\n",
    "for i in range(1):#range(config['num_clients']):\n",
    "\n",
    "    head = CNN_Head(mode)\n",
    "    neck = CNN_Neck(mode)\n",
    "\n",
    "    neck.load_weights(neck_weights)\n",
    "    head.load_weights(head_weights)\n",
    "\n",
    "    loader = Dataloader(file_indices,[i*(client_slice),min((i+1)*(client_slice),35)])    \n",
    "    loader.load_training_dataset()\n",
    "    loader.load_testing_dataset()\n",
    "    threads.append(Client(str(i),\n",
    "                          neck,\n",
    "                          head,\n",
    "                          upload_schedule,\n",
    "                          model_queue,\n",
    "                          param_queue,\n",
    "                          loader,\n",
    "                          data_manager = dm))\n",
    "    \n",
    "    \n",
    "    dataset_split.append([i*(client_slice),min((i+1)*(client_slice),35)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loader.load_batch(batch_size=2)['input'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Old Weights for Each Client\n",
    "\n",
    "Load the pre-trained weights to each of the clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_path = exp_path + \"individual_head_networks_\"\n",
    "for i in range(len(threads)):\n",
    "    head_path = nn_path + str(i) +\"_head_network\"\n",
    "    neck_path = nn_path + str(i) +\"_neck_network\"\n",
    "    \n",
    "    head = torch.load(head_path)\n",
    "    neck = torch.load(neck_path)\n",
    "    \n",
    "    head_edit = OrderedDict()\n",
    "    neck_edit = OrderedDict()\n",
    "\n",
    "    # Edit the ordered_dict key names to be torch compatible\n",
    "    for key in head.keys():\n",
    "        head_edit[\"network.\"+key] = head[key]\n",
    "\n",
    "    for key in neck.keys():\n",
    "        neck_edit[\"network.\"+key] = neck[key]\n",
    "    \n",
    "    threads[i].head.load_state_dict(head_edit)\n",
    "    threads[i].neck.load_state_dict(neck_edit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Results for Each Client\n",
    "\n",
    "Run through each of the clients and print their test value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 -- Train Acc: 0.899  -- Test Acc:  0.796\n",
      "Client 1 -- Train Acc: 0.875  -- Test Acc:  0.739\n",
      "Client 2 -- Train Acc: 0.864  -- Test Acc:  0.789\n",
      "Client 3 -- Train Acc: 0.887  -- Test Acc:  0.757\n",
      "Client 4 -- Train Acc: 0.899  -- Test Acc:  0.617\n",
      "Client 5 -- Train Acc: 0.866  -- Test Acc:  0.802\n",
      "Client 6 -- Train Acc: 0.874  -- Test Acc:  0.724\n",
      "Client 7 -- Train Acc: 0.856  -- Test Acc:  0.795\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(threads)):\n",
    "    threads[i].testing_check(None)\n",
    "    threads[i].training_check()\n",
    "    print(\"Client\", i, \"-- Train Acc:\", round(threads[i].training_accuracy,3) ,\n",
    "          \" -- Test Acc: \", round(threads[i].testing_accuracy,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Swap Training and Test Sets for Each Client\n",
    "\n",
    "To observe a sense of \"Transferability\" by observing test and training acurracies with datasets from different clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  all_data_12_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_20_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_11_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_18_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_0_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_34_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_17_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_13_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_7_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_33_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_24_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_5_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_27_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_26_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_21_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_10_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_19_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_6_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_32_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_15_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_2_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_3_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_31_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_30_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_16_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_8_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_25_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_1_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_14_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_28_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_9_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_22_niid_0_keep_0_train_9.json\n"
     ]
    }
   ],
   "source": [
    "swap_training_acc = np.zeros((len(threads),len(threads)))\n",
    "swap_test_acc = np.zeros((len(threads),len(threads)))\n",
    "\n",
    "for src in range(len(threads)):\n",
    "    \n",
    "    head = CNN_Head(mode)\n",
    "    neck = CNN_Neck(mode)\n",
    "\n",
    "    neck.load_weights(neck_weights)\n",
    "    head.load_weights(head_weights)\n",
    "    \n",
    "    loader = Dataloader(file_indices,[src*(client_slice),min((src+1)*(client_slice),35)])    \n",
    "    loader.load_training_dataset()\n",
    "    loader.load_testing_dataset()\n",
    "    curr_client = Client('dummy',\n",
    "                          neck,\n",
    "                          head,\n",
    "                          upload_schedule,\n",
    "                          model_queue,\n",
    "                          param_queue,\n",
    "                          loader,\n",
    "                          data_manager = dm)\n",
    "    \n",
    "    for dest in range(len(threads)):  \n",
    "    \n",
    "        # Load current weights for destination\n",
    "        head_path = nn_path + str(dest) +\"_head_network\"\n",
    "        neck_path = nn_path + str(dest) +\"_neck_network\"\n",
    "\n",
    "        head = torch.load(head_path)\n",
    "        neck = torch.load(neck_path)\n",
    "\n",
    "        head_edit = OrderedDict()\n",
    "        neck_edit = OrderedDict()\n",
    "\n",
    "        # Edit the ordered_dict key names to be torch compatible\n",
    "        for key in head.keys():\n",
    "            head_edit[\"network.\"+key] = head[key]\n",
    "\n",
    "        for key in neck.keys():\n",
    "            neck_edit[\"network.\"+key] = neck[key]\n",
    "\n",
    "        curr_client.head.load_state_dict(head_edit)\n",
    "        curr_client.neck.load_state_dict(neck_edit)\n",
    "\n",
    "        curr_client.testing_check(None)\n",
    "        curr_client.training_check()\n",
    "\n",
    "        swap_training_acc[src,dest] = curr_client.training_accuracy\n",
    "        swap_test_acc[src,dest] = curr_client.testing_accuracy\n",
    "\n",
    "        dataset_split.append([i*(client_slice),min((i+1)*(client_slice),35)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy Transfer\n",
      "[[0.796 0.791 0.778 0.767 0.72  0.776 0.777 0.798]\n",
      " [0.751 0.739 0.72  0.749 0.651 0.721 0.723 0.777]\n",
      " [0.763 0.782 0.789 0.726 0.753 0.77  0.778 0.778]\n",
      " [0.787 0.797 0.776 0.757 0.745 0.779 0.78  0.788]\n",
      " [0.809 0.734 0.705 0.828 0.617 0.71  0.705 0.81 ]\n",
      " [0.756 0.799 0.809 0.692 0.829 0.802 0.822 0.765]\n",
      " [0.771 0.746 0.726 0.777 0.656 0.743 0.724 0.784]\n",
      " [0.779 0.765 0.759 0.769 0.688 0.754 0.76  0.795]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy Transfer\")\n",
    "print(np.round(swap_test_acc,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Transfer\n",
      "[[0.898 0.805 0.782 0.841 0.737 0.782 0.793 0.839]\n",
      " [0.784 0.875 0.809 0.739 0.795 0.815 0.815 0.791]\n",
      " [0.716 0.786 0.86  0.662 0.783 0.791 0.799 0.761]\n",
      " [0.819 0.743 0.709 0.888 0.6   0.716 0.707 0.825]\n",
      " [0.744 0.819 0.822 0.689 0.897 0.82  0.83  0.773]\n",
      " [0.741 0.804 0.804 0.69  0.79  0.862 0.805 0.765]\n",
      " [0.753 0.806 0.81  0.71  0.795 0.803 0.875 0.783]\n",
      " [0.781 0.757 0.747 0.78  0.666 0.742 0.744 0.857]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Accuracy Transfer\")\n",
    "print(np.round(swap_training_acc,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
