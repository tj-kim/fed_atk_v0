{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Characterizing Datasets\n",
    "\n",
    "TJ Kim <br/>\n",
    "3/14/21\n",
    "\n",
    "Updated <br/>\n",
    "3/14/21\n",
    "\n",
    "#### Objective: \n",
    "Characterize dataset that are not i.i.d distributed: <br/>\n",
    "\n",
    "Deliverables\n",
    "- Guassian extrapolation of mean and variance of each \"feature node\"\n",
    "- PAC representation of the point clusters \n",
    "\n",
    "The different clusters will be divided along:\n",
    "- nth data set for each client\n",
    "- across class lines\n",
    "- combination of 2 prior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd '/home/ubuntu/FedAtk/' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Relevant Libraries and Modules\n",
    "\n",
    "Load the relevant libraries for the federated learning code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transferer\n",
    "from transfer_attacks.Transferer import *\n",
    "from configs.overwrite_config import *\n",
    "\n",
    "# General Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from federated_training.femnist_dataloader import Dataloader\n",
    "from federated_training.cnn_head import CNN_Head\n",
    "from federated_training.cnn_neck import CNN_Neck\n",
    "from federated_training.cnn_server import Server\n",
    "from federated_training.cnn_client import Client\n",
    "from federated_training.data_manager import DataManager\n",
    "from federated_training.utils import cuda, where\n",
    "from federated_training.utilities import freeze_layers\n",
    "\n",
    "from cw_attack.cw import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "import os\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import copy\n",
    "\n",
    "# Import Relevant Libraries\n",
    "from transfer_attacks.Transferer import *\n",
    "\n",
    "class DA_Transferer(Transferer): \n",
    "    \"\"\"\n",
    "    - Load all the datasets but separate them\n",
    "    - Intermediate values of featues after 2 convolution layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filename:str, config_name = None):\n",
    "        super(DA_Transferer, self).__init__(filename=filename, config_name=config_name)\n",
    "        \n",
    "        # Hold Onto the data\n",
    "        self.DA_x = {} # Indexed by client id of dataset\n",
    "        self.DA_y = {} # Also can be indexed by class - double dictionary for class-client pair\n",
    "        self.DA_intermed = {}\n",
    "        self.loader_i = {}\n",
    "        \n",
    "        # Data division information\n",
    "        self.mode = None\n",
    "        self.client_idxs = None\n",
    "        self.classes = None\n",
    "        \n",
    "        # Gaussian Extraction\n",
    "        self.gaussian_ustd= {}\n",
    "        \n",
    "        # PCA Extraction\n",
    "        self.PCA_data= {}\n",
    "        self.dimension = None\n",
    "        self.explained_var_ratio = None\n",
    "        \n",
    "        \n",
    "    def load_niid_data(self, clients = [0,1,2,3,4,5,6,7]):\n",
    "        \"\"\"\n",
    "        Store all data in dictionary (pre-load) separated by client idx\n",
    "        \"\"\"\n",
    "        \n",
    "        self.client_idxs = clients\n",
    "        \n",
    "        # Import Data Loader for this FL set\n",
    "        file_indices = [i for i in range(self.config['num_sets'])]\n",
    "        client_slice = len(file_indices)//self.config['num_clients']\n",
    "        \n",
    "        for client_idx in clients:\n",
    "            self.loader_i[client_idx] = Dataloader(file_indices,[client_idx*(client_slice),min((client_idx+1)*(client_slice),35)])  \n",
    "            self.loader_i[client_idx].load_training_dataset()\n",
    "            self.loader_i[client_idx].load_testing_dataset()\n",
    "        \n",
    "         \n",
    "    def set_data(self, mode='client', datasets = range(8), batch_size = 50, classes = [0,1]):\n",
    "        \"\"\"\n",
    "        - fill DA_x, DA_y with relevant data according to dictionary\n",
    "        modes:\n",
    "            - 'client' - load all data for specified clients without class filtering\n",
    "            - 'class'  - load all data and filters by class for different classes separately for single client\n",
    "            - 'both'   - load all data and filters by class for different classes separately for multiple\n",
    "        datasets:\n",
    "            which clients to take dataset from\n",
    "        \"\"\"\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.client_idxs = datasets\n",
    "        self.classes = classes\n",
    "        self.DA_x = {} # Reset\n",
    "        self.DA_y = {}\n",
    "        \n",
    "        \n",
    "        # store data differently based on what the desired mode is\n",
    "        if mode == 'client':\n",
    "            for i in datasets:\n",
    "                image_data = self.loader_i[i].load_batch(batch_size, mode='test')\n",
    "                self.DA_x[i] = torch.Tensor(image_data['input']).reshape(batch_size,1,28,28) # Eliminate magic number\n",
    "                self.DA_y[i] = torch.Tensor(image_data['label']).type(torch.LongTensor)\n",
    "            \n",
    "        elif mode == 'class':\n",
    "            idx = datasets[0] # If given multiple classes take the first one\n",
    "            loader = self.loader_i[idx]\n",
    "            y = np.array(loader.test_dataset['user_data']['y'])\n",
    "            for c in classes:\n",
    "                args = np.argwhere(y==c)\n",
    "                np.random.shuffle(args)\n",
    "                \n",
    "                # If not enough samples\n",
    "                if args.shape[0] < batch_size:\n",
    "                    batch_size_temp = args.shape[0]\n",
    "                else: \n",
    "                    batch_size_temp = batch_size\n",
    "                \n",
    "                args = args[0:batch_size_temp]\n",
    "                args = args.ravel()\n",
    "                \n",
    "                # Append data point one by one\n",
    "                self.DA_x[c] = torch.Tensor(np.array(loader.test_dataset['user_data']['x'])[args]).reshape(batch_size_temp,1,28,28)\n",
    "                self.DA_y[c] = torch.Tensor(np.array(loader.test_dataset['user_data']['y'])[args])\n",
    "        \n",
    "        elif mode == 'both':\n",
    "            for i in datasets:\n",
    "                loader = self.loader_i[i]\n",
    "                y = np.array(loader.test_dataset['user_data']['y'])\n",
    "                \n",
    "                self.DA_x[i] = {}\n",
    "                self.DA_y[i] = {}\n",
    "                \n",
    "                for c in classes:\n",
    "                    args = np.argwhere(y==c)\n",
    "                \n",
    "                    # If not enough samples\n",
    "                    if args.shape[0] < batch_size:\n",
    "                        batch_size_temp = args.shape[0]\n",
    "                    else: \n",
    "                        batch_size_temp = batch_size\n",
    "\n",
    "                    args = args[0:batch_size_temp]\n",
    "                    args = args.ravel()\n",
    "                    self.DA_x[i][c] = torch.Tensor(np.array(loader.test_dataset['user_data']['x'])[args]).reshape(batch_size_temp,1,28,28)\n",
    "                    self.DA_y[i][c] = torch.Tensor(np.array(loader.test_dataset['user_data']['y'])[args])\n",
    "                    \n",
    "        else:\n",
    "            raise Exception(\"Invalid data analysis mode\") \n",
    "        \n",
    "    def forward_neck(self, x):\n",
    "        \"\"\"\n",
    "        Only forward through neck to get upto intermediate flattened layer\n",
    "        \"\"\"\n",
    "    \n",
    "        if torch.cuda.is_available():\n",
    "                x = x.cuda()\n",
    "        \n",
    "        x = self.advNN.neck.forward(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def forward_pass(self):\n",
    "        \n",
    "        # Turn off dropout \n",
    "        self.advNN.eval()\n",
    "        \n",
    "        self.DA_intermed = {}\n",
    "\n",
    "        if self.mode == 'client' or self.mode == 'class':\n",
    "            for client_idx, value in self.DA_x.items():\n",
    "                self.DA_intermed[client_idx] = self.forward_neck(value)\n",
    "\n",
    "        elif self.mode == 'both':\n",
    "            for client_idx, classes in self.DA_x.items():\n",
    "                self.DA_intermed[client_idx] = {}\n",
    "                for class_idx, value in classes.items():\n",
    "                    self.DA_intermed[client_idx][class_idx] = self.forward_neck(value)\n",
    "    \n",
    "    \n",
    "    def obtain_gaussian(self):\n",
    "        \"\"\"\n",
    "        Mean and standard deviation log of every data set split\n",
    "        \"\"\"\n",
    "        \n",
    "        # Reset Dictionary\n",
    "        self.gaussian_ustd= {}\n",
    "        \n",
    "        if self.mode == 'client':\n",
    "            self.gaussian_ustd['info'] = (\"mean\",\"std\",\"client\")\n",
    "            for client_idx in self.client_idxs:\n",
    "                self.gaussian_ustd[client_idx] = {}\n",
    "                data = transferer.DA_intermed[client_idx]\n",
    "                self.gaussian_ustd[client_idx]['mean'] = torch.mean(data,0)\n",
    "                self.gaussian_ustd[client_idx]['std'] = torch.std(data,0)\n",
    "                \n",
    "        elif self.mode == 'class':\n",
    "            self.gaussian_ustd['info'] = (\"mean\",\"std\",\"class\")\n",
    "            for class_idx in self.classes:\n",
    "                self.gaussian_ustd[class_idx] = {}\n",
    "                data = transferer.DA_intermed[class_idx]\n",
    "                self.gaussian_ustd[class_idx]['mean'] = torch.mean(data,0)\n",
    "                self.gaussian_ustd[class_idx]['std'] = torch.std(data,0)\n",
    "                \n",
    "        elif self.mode == 'both':\n",
    "            self.gaussian_ustd['info'] = (\"mean\",\"std\",\"both\")\n",
    "            for client_idx in self.client_idxs:\n",
    "                self.gaussian_ustd[client_idx] = {}\n",
    "                for class_idx in self.classes:\n",
    "                    self.gaussian_ustd[client_idx][class_idx] = {}\n",
    "                    data = transferer.DA_intermed[client_idx][class_idx]\n",
    "                    self.gaussian_ustd[client_idx][class_idx]['mean'] = torch.mean(data,0)\n",
    "                    self.gaussian_ustd[client_idx][class_idx]['std'] = torch.std(data,0)\n",
    "    \n",
    "    def obtain_PCA(self, dim=2):\n",
    "        \"\"\"\n",
    "        Dimension reduction per data point from 400 --> 2 for all data points\n",
    "        https://github.com/mGalarnyk/Python_Tutorials/blob/master/Sklearn/PCA/PCA_Data_Visualization_Iris_Dataset_Blog.ipynb\n",
    "        \"\"\"\n",
    "        # Reset Dictionary\n",
    "        self.PCA_data = {}\n",
    "        self.dimension = dim\n",
    "        \n",
    "        output_dim = 400 # Eliminate magic numbers\n",
    "        data = np.empty([0,output_dim])\n",
    "        \n",
    "        # Convert data to numpy and gather them together \n",
    "        # Keep index of each class pair in dictionary\n",
    "        if self.mode == 'client':\n",
    "            indices = np.empty(0)\n",
    "            for client_idx in self.client_idxs:\n",
    "                new_data = transferer.DA_intermed[client_idx].cpu().detach().numpy()\n",
    "                data = np.append(data, new_data, axis=0)\n",
    "                index = np.ones(new_data.shape[0]) * client_idx\n",
    "                indices = np.append(indices, index,axis=0)\n",
    "                \n",
    "        elif self.mode == 'class':\n",
    "            indices = np.empty(0)\n",
    "            for class_idx in self.classes:\n",
    "                new_data = transferer.DA_intermed[class_idx].cpu().detach().numpy()\n",
    "                data = np.append(data, new_data,axis=0)\n",
    "                index = np.ones(new_data.shape[0]) * class_idx\n",
    "                indices = np.append(indices, index,axis=0)\n",
    "                \n",
    "        elif self.mode == 'both':\n",
    "            indices = np.empty((0,2))\n",
    "            for client_idx in self.client_idxs:\n",
    "                for class_idx in self.classes:\n",
    "                    new_data = transferer.DA_intermed[client_idx][class_idx].cpu().detach().numpy()\n",
    "                    data = np.append(data,new_data,axis=0)\n",
    "                    index = np.append(np.ones([new_data.shape[0],1])*client_idx, np.ones([new_data.shape[0],1])*class_idx,axis=1)\n",
    "                    indices = np.append(indices,index,axis=0)\n",
    "        \n",
    "        \n",
    "        # Standardize across dimensions of data\n",
    "        data = StandardScaler().fit_transform(data)\n",
    "        \n",
    "        # Run PCA on data with dimension \n",
    "        pca = PCA(n_components=dim)\n",
    "        principalComponents = pca.fit_transform(data)\n",
    "        self.explained_var_ratio = pca.explained_variance_ratio_\n",
    "        \n",
    "        self.PCA_data['data'] = principalComponents\n",
    "        self.PCA_data['labels'] = indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory in results for this experiment\n",
    "# FL Architecture\n",
    "client_idx = 1\n",
    "victim_idxs = [0,1,2,3]\n",
    "\n",
    "# Saved Neural Networks to Test on \n",
    "exp_names = [\"exp4_neck2_0_head3\"]\n",
    "\n",
    "# Parameters to record for excel printing\n",
    "num_clients = len(victim_idxs)\n",
    "metrics = ['orig_acc','orig_sim','orig_acc_robust', 'orig_sim_robust', \n",
    "           'orig_acc_adv', 'orig_sim_adv','adv_sim','adv_hit','g_align',\n",
    "           'g_align_robust', 'g_align_adv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  all_data_0_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_34_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_17_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_13_niid_0_keep_0_train_9.json\n",
      "generated model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.8/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "transferer = DA_Transferer(filename = exp_names[0])\n",
    "transferer.generate_advNN(client_idx = client_idx)\n",
    "transferer.generate_victims(client_idxs = victim_idxs)\n",
    "print('generated model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading  all_data_12_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_20_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_11_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_18_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_0_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_34_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_17_niid_0_keep_0_train_9.json\n",
      "Loading  all_data_13_niid_0_keep_0_train_9.json\n"
     ]
    }
   ],
   "source": [
    "transferer.load_niid_data(clients=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transferer.set_data(mode='both', datasets = range(2), batch_size = 20, classes = [0,1])\n",
    "transferer.forward_pass()\n",
    "transferer.obtain_PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transferer.PCA_data['labels'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
